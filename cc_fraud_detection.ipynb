{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d2698b-7d95-497f-8d79-87170d247990",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-Time Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:19:34.990729Z",
     "start_time": "2023-11-21T14:19:34.979479Z"
    }
   },
   "source": [
    "## Activity List\n",
    "\n",
    "This is a dump of proposed activities. Feel absolutely free to create and take\n",
    "ownership of any activity you may think of!\n",
    "\n",
    "| Activity | Owner   | Status |\n",
    "| :-------- | :-------: | :------: |\n",
    "| EDA - Fraud as a function of hour of day  |   ||\n",
    "| EDA - Fraud per category |   ||\n",
    "| EDA - Fraud per vendor | ||\n",
    "| Feature - # of transactions last 24 hrs  |    ||\n",
    "| Feature - total amount for card last 24 hrs | ||\n",
    "| Feature - avg amount for card last 24 hrs | ||\n",
    "| Feature - # of fraud occurences for vendor | ||\n",
    "| ML - pipeline | ||\n",
    "| ML - streaming | ||\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e026d554-cc9a-4ac3-9fe9-f4b94c6e1642",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:13.136699Z",
     "start_time": "2023-11-21T14:48:12.024676Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac2b174-0651-4698-a437-545dda582df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import modules from pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# uncomment the following line if running pyspark from the notebook itself\n",
    "# spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4fa66b5-0bd7-42b2-8002-0deffa9b601f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the Dataset File and Performing Basic Data Type Conversions\n",
    "\n",
    "Source of the dataset: https://www.kaggle.com/datasets/kartik2112/fraud-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:14.723541Z",
     "start_time": "2023-11-21T14:48:14.715316Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a reusable schema for the dataset (will be useful for the real-time portion)\n",
    "ccschema = StructType([\n",
    "    StructField(\"_c0\", IntegerType(), True),\n",
    "    StructField(\"trans_date_trans_time\", TimestampType(), True),\n",
    "    StructField(\"cc_num\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", DoubleType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"unix_time\", StringType(), True),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:17.257898Z",
     "start_time": "2023-11-21T14:48:15.679110Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78258743-1bd7-48ea-b54a-82f0637471e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data from local folder\n",
    "cc = (spark.read.csv(\"fraudTrain.csv\", schema=ccschema, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:18.198665Z",
     "start_time": "2023-11-21T14:48:18.195661Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662b3a2b-365c-4f39-8764-5d80d45093a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data in the context of Databricks\n",
    "# cc = (spark.read\n",
    "#  .option(\"header\", \"true\")\n",
    "#  .csv(\"s3://group9-ml-project/fraudTrain.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:19.881120Z",
     "start_time": "2023-11-21T14:48:19.878127Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this code to create some sample data from the main dataset for the purpose\n",
    "# of loading transactions for the real time portion\n",
    "\n",
    "# cc.limit(10).write.option(\"header\",True).csv(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750eac0b-4eb7-48ad-961c-0ef45e6c85ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b6fbbf-237e-4274-8e14-4ba90030b5ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A Look at the Data and its Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:24.849346Z",
     "start_time": "2023-11-21T14:48:23.128917Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae7e13a-f45b-4923-96bb-dfb19a7af27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's look a the first 5 rows\n",
    "pd.DataFrame(cc.take(5), columns=cc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:25:02.836299Z",
     "start_time": "2023-11-21T14:24:44.078099Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd519a3e-30d3-4409-9fe2-dadf0bf338a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# basic statistics\n",
    "cc.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:30:01.194781Z",
     "start_time": "2023-11-21T14:29:56.764645Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf09b72-f077-4f6c-9f08-1978f21b42dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# looking to see if there are null values\n",
    "cc.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in cc.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84faa957-4245-4c35-b435-6672c292af11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualizing the Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:30:58.242101Z",
     "start_time": "2023-11-21T14:30:19.223208Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352d619f-99a1-492c-8b18-9cebef04f7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4 , 2, figsize=(15, 20))\n",
    "fig.suptitle('CC Fraud Data Distribution')\n",
    "\n",
    "for idx, column in enumerate(['amt', 'city_pop', 'lat', 'long', 'merch_lat', 'merch_long', 'unix_time', 'is_fraud']):\n",
    "    # Show histogram of the column\n",
    "    bins, counts = cc.select(column).rdd.flatMap(lambda x: x).map(float).histogram(20)\n",
    "    axs[idx//2][idx%2].set_title(column)\n",
    "    axs[idx//2][idx%2].hist(bins[:-1], bins=bins, weights=counts)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:31:26.938275Z",
     "start_time": "2023-11-21T14:31:25.649593Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a728cb02-c2c8-4a85-92dc-0cd5ee9e0964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"category\").groupby(\"category\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:31:31.063417Z",
     "start_time": "2023-11-21T14:31:30.144562Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d4906c-1a4f-4326-a063-ce1939ce9260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"gender\").groupby(\"gender\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fd7996-c3bd-4d19-8ac5-d1dfad4dc8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EDA Preliminary Findings\n",
    "\n",
    "- The data appears to be clean with no missing values\n",
    "- Some of the heavily skewed features like amt and city_pop may benefit from logarithmic transformation\n",
    "- The target class (is_fraud) is heavily imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "554b951f-367b-40b0-bad2-07dfa6bd2870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:16.350014Z",
     "start_time": "2023-11-21T14:47:16.339658Z"
    }
   },
   "outputs": [],
   "source": [
    "# define logarithmic transformer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only  # Note: use pyspark.ml.util.keyword_only if Spark < 2.0\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    " \n",
    "class LogTransformer(Transformer,               # Base class\n",
    "                     HasInputCol,               # Sets up an inputCol parameter\n",
    "                     HasOutputCol,              # Sets up an outputCol parameter\n",
    "                     DefaultParamsReadable,     # Makes parameters readable from file\n",
    "                     DefaultParamsWritable      # Makes parameters writable from file\n",
    "                    ):\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, append_str=None):\n",
    "        \"\"\"\n",
    "        Constructor: set values for all Param objects\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._setDefault()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "  \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        This is the main member function which applies the transform to transform data from the `inputCol` to the `outputCol`\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"LogTransformer transformer.\"\n",
    "            )\n",
    "        input_column = self.getInputCol()\n",
    "        output_column = self.getOutputCol()\n",
    "\n",
    "        return dataset.withColumn(output_column,\n",
    "                                  log(col(input_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:17.817197Z",
     "start_time": "2023-11-21T14:47:17.790339Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61929bd3-d3ca-4ead-8355-c7aa8510fff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# define a transformer to convert string categorical features to numeric indices\n",
    "inputs = ['merchant', 'category', 'gender', 'city', 'state', 'job']\n",
    "outputs = ['merchant_idx', 'category_idx', 'gender_idx', 'city_idx', 'state_idx', 'job_idx']\n",
    "stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:19.470612Z",
     "start_time": "2023-11-21T14:47:19.459432Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c330f6-3299-4f01-8be9-e093e31e0201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# define a transformer to one-hot encode indexed categorical features\n",
    "inputs_1hot = ['merchant_idx', 'category_idx', 'city_idx', 'state_idx', 'job_idx']\n",
    "outputs_1hot = ['merchant_1hot', 'category_1hot', 'city_1hot', 'state_1hot', 'job_1hot']\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=inputs_1hot, outputCols=outputs_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:20.898896Z",
     "start_time": "2023-11-21T14:47:20.888356Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7f0afc-bb1a-400e-8381-803b5d29655b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assemble the prepped features into one single vector.\n",
    "featureCols = ['amt_log', 'city_pop_log', 'job_1hot', 'state_1hot', 'category_1hot', 'gender_idx']\n",
    "assembler = (VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\"))\n",
    "\n",
    "# cc_final = assembler.transform(cc_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:24.083278Z",
     "start_time": "2023-11-21T14:47:24.079561Z"
    }
   },
   "outputs": [],
   "source": [
    "amtTransformer = LogTransformer(inputCol=\"amt\", outputCol=\"amt_log\")\n",
    "cityPopTransformer = LogTransformer(inputCol=\"city_pop\", outputCol=\"city_pop_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac03ef3e-ea18-4b83-b2f4-d815c04e81ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML Training and Prediction - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = cc.randomSplit([0.7, 0.3])\n",
    "\n",
    "print(training.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:36:35.187683Z",
     "start_time": "2023-11-15T19:36:10.439863Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862b103f-7fa2-4e8f-bb2c-392d684e3cff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"is_fraud\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "rf.setFeaturesCol(\"features\")\n",
    "\n",
    "# define pipeline using previously defined stages\n",
    "pipeline = Pipeline(stages=[stringIndexer, oneHotEncoder, amtTransformer, cityPopTransformer, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:37:45.836414Z",
     "start_time": "2023-11-15T19:37:45.760815Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ae490b-dd6a-4a8d-a9c2-1a07021da0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = model.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:39:59.731216Z",
     "start_time": "2023-11-15T19:39:57.820743Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692188e6-9c49-497b-8b56-b04bd83147e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds.take(5), columns=preds.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:44:25.045325Z",
     "start_time": "2023-11-15T19:44:19.726175Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c3bac3-aaeb-40d5-a104-254c44cab3b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Instantiate the evaluator\n",
    "bce= BinaryClassificationEvaluator(rawPredictionCol= \"rawPrediction\",\n",
    "                                   labelCol=\"is_fraud\", \n",
    "                                   metricName= \"areaUnderROC\")\n",
    "                                   \n",
    "bce.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T20:26:50.892046Z",
     "start_time": "2023-11-15T20:26:38.182845Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c331dd2d-7945-4a22-98f0-f6569566f8e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "preds_float = preds \\\n",
    "    .select(\"prediction\", \"is_fraud\") \\\n",
    "    .withColumn(\"is_fraud\", col(\"is_fraud\").cast(DoubleType())) \\\n",
    "    .orderBy(\"prediction\")\n",
    "\n",
    "cm = MulticlassMetrics(preds_float.rdd.map(tuple))\n",
    "\n",
    "# print(cm.confusionMatrix().toArray())\n",
    "\n",
    "#show the confusion matrix as a pandas df for clearer presentation\n",
    "pd.DataFrame(cm.confusionMatrix().toArray(),\n",
    "             columns= [\"true positive\", \"true negative\"],\n",
    "             index= [\"predicted positive\", \"predicted negative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74111b8-28b8-432c-9013-c8a482d70d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results Analysis\n",
    "\n",
    "With a false negative rate of 100% in the confusion matrix, and 0.5 AUC score we obviously have work to do! ;-)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cc_fraud_detection",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
