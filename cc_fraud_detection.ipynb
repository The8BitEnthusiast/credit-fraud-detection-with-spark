{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d2698b-7d95-497f-8d79-87170d247990",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-Time Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e026d554-cc9a-4ac3-9fe9-f4b94c6e1642",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:13.136699Z",
     "start_time": "2023-11-21T14:48:12.024676Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac2b174-0651-4698-a437-545dda582df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import modules from pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# uncomment the following line if running pyspark from the notebook itself\n",
    "# spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4fa66b5-0bd7-42b2-8002-0deffa9b601f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the Dataset File and Performing Basic Data Type Conversions\n",
    "\n",
    "Source of the dataset: https://www.kaggle.com/datasets/kartik2112/fraud-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:14.723541Z",
     "start_time": "2023-11-21T14:48:14.715316Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a reusable schema for the dataset (will be useful for the real-time portion)\n",
    "ccschema = StructType([\n",
    "    StructField(\"_c0\", IntegerType(), True),\n",
    "    StructField(\"trans_date_trans_time\", TimestampType(), True),\n",
    "    StructField(\"cc_num\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", DoubleType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"unix_time\", StringType(), True),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:17.257898Z",
     "start_time": "2023-11-21T14:48:15.679110Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78258743-1bd7-48ea-b54a-82f0637471e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data from local folder\n",
    "cc = (spark.read.csv(\"fraudTrain.csv\", schema=ccschema, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:18.198665Z",
     "start_time": "2023-11-21T14:48:18.195661Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662b3a2b-365c-4f39-8764-5d80d45093a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data in the context of Databricks\n",
    "# cc = (spark.read.csv(\"s3://group9-ml-project/fraudTrain.csv\", schema=ccschema, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:19.881120Z",
     "start_time": "2023-11-21T14:48:19.878127Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this code to create some sample data from the main dataset for the purpose\n",
    "# of loading transactions for the real time portion\n",
    "\n",
    "# cc.limit(2).write.option(\"header\",False).csv(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750eac0b-4eb7-48ad-961c-0ef45e6c85ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b6fbbf-237e-4274-8e14-4ba90030b5ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A Look at the Data and its Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:48:24.849346Z",
     "start_time": "2023-11-21T14:48:23.128917Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae7e13a-f45b-4923-96bb-dfb19a7af27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's look a the first 5 rows\n",
    "cc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:25:02.836299Z",
     "start_time": "2023-11-21T14:24:44.078099Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd519a3e-30d3-4409-9fe2-dadf0bf338a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# basic statistics\n",
    "cc.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:30:01.194781Z",
     "start_time": "2023-11-21T14:29:56.764645Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf09b72-f077-4f6c-9f08-1978f21b42dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# looking to see if there are null values\n",
    "cc.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in cc.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84faa957-4245-4c35-b435-6672c292af11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualizing the Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:30:58.242101Z",
     "start_time": "2023-11-21T14:30:19.223208Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352d619f-99a1-492c-8b18-9cebef04f7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4 , 2, figsize=(15, 20))\n",
    "fig.suptitle('CC Fraud Data Distribution')\n",
    "\n",
    "for idx, column in enumerate(['amt', 'city_pop', 'lat', 'long', 'merch_lat', 'merch_long', 'unix_time', 'is_fraud']):\n",
    "    # Show histogram of the column\n",
    "    bins, counts = cc.select(column).rdd.flatMap(lambda x: x).map(float).histogram(20)\n",
    "    axs[idx//2][idx%2].set_title(column)\n",
    "    axs[idx//2][idx%2].hist(bins[:-1], bins=bins, weights=counts)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:31:26.938275Z",
     "start_time": "2023-11-21T14:31:25.649593Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a728cb02-c2c8-4a85-92dc-0cd5ee9e0964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"category\").groupby(\"category\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:31:31.063417Z",
     "start_time": "2023-11-21T14:31:30.144562Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d4906c-1a4f-4326-a063-ce1939ce9260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"gender\").groupby(\"gender\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud Ratio by Age Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is fraud more prevalent in certain age groups than others? The analysis below suggests that seniors aged 80+ are more exposed to fraudulent transactions than other groups. This makes age group a potentially viable feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate age group from date of birth. Notice the use of the floor function, which\n",
    "# means that a person in the 30-40 age group would be deemed belonging to the \"30\" group\n",
    "cc_age = cc.withColumn(\"age_group\", \n",
    "                       floor(months_between(current_date(), \n",
    "                        col(\"dob\"))/12/10)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram of fraudulent transactions by age group\n",
    "bins, counts = (cc_age.where(col(\"is_fraud\")==1.0)\n",
    "                      .select(\"age_group\").rdd.flatMap(lambda x: x)\n",
    "                               .map(float).histogram(list(range(0,110,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram of non-fraudulent transactions by age group\n",
    "bins2, counts2 = (cc_age.where(col(\"is_fraud\")==0.0)\n",
    "                      .select(\"age_group\").rdd.flatMap(lambda x: x)\n",
    "                               .map(float).histogram(list(range(0,110,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ratios between fraudulent and non-fraudulent transactions\n",
    "def safediv(arg1, arg2):\n",
    "    return arg1 / arg2 if (arg2 != 0) else 0;\n",
    "\n",
    "ratios = list(map(safediv, counts, counts2))\n",
    "# ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "hist = pd.DataFrame(zip(bins,ratios), columns=['age_group','fraud_ratio'])\n",
    "sns.barplot(hist, x=\"age_group\", y=\"fraud_ratio\").set(title='Fraud Ratio by Age Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fd7996-c3bd-4d19-8ac5-d1dfad4dc8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EDA Preliminary Findings\n",
    "\n",
    "- The data appears to be clean with no missing values\n",
    "- Some of the heavily skewed features like amt and city_pop may benefit from logarithmic transformation\n",
    "- The target class (is_fraud) is heavily imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last 24 Hour Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txByWindow = cc.groupby(\"cc_num\",window(cc.trans_date_trans_time, \"1 hour\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(txByWindow.take(20), columns=txByWindow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_window = cc.withColumn(\"window\", window(cc.trans_date_trans_time, \"1 hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_joined = cc_window.join(txByWindow, ['cc_num', 'window'], \"outer\").withColumnRenamed(\"count\", \"fraud_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(cc_joined.where(col(\"is_fraud\")==1).take(40), cc_joined.columns)\n",
    "\n",
    "cc_fraud_counts = cc_joined.where(col(\"is_fraud\")==1).select(\"fraud_count\").groupby(\"fraud_count\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_fraud_counts.orderBy(\"fraud_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc_sl = cc.withColumn('ts_seconds', col(\"trans_date_trans_time\").cast('long'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "\n",
    "#w = (Window()\n",
    "#     .partitionBy(col(\"cc_num\"))\n",
    "#     .orderBy('ts_seconds')\n",
    "#     .rangeBetween(-60*60*24, Window.currentRow)\n",
    "#     )\n",
    "\n",
    "#df1 = (cc_sl\n",
    "#       .withColumn('txns_last24', count(\"*\").over(w))\n",
    "#       .orderBy(desc(\"ts_seconds\"))\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_window = cc.withColumn('window', window(col(\"trans_date_trans_time\"), \"24 hours\", \"1 hour\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by window and word and compute the count of each group\n",
    "windowedCounts = cc.groupBy(\n",
    "    window(cc.trans_date_trans_time, \"24 hours\", \"1 hour\"),\n",
    "    cc.cc_num\n",
    ").agg(count('cc_num').alias('txns_last24'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_counts = cc_window.join(windowedCounts, [\"window\", \"cc_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cc_counts.take(10), columns=cc_counts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df1.where(col(\"txns_last_hour\")>3).take(20), columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = (df1\n",
    "#       .withColumn('amt_last24', sum(\"amt\").over(w))\n",
    "#       .orderBy(desc(\"amt\"))\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(df2.where(col(\"txns_last_hour\")>3).take(20), columns=df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance from Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use haversine formula to calculate distance from home\n",
    "cc_dist = cc.withColumn('dist_kms' , \\\n",
    "            round((acos((sin(radians(col(\"lat\"))) * sin(radians(col(\"merch_lat\")))) + \\\n",
    "                   ((cos(radians(col(\"lat\"))) * cos(radians(col(\"merch_lat\")))) * \\\n",
    "                    (cos(radians(col(\"long\")) - radians(col(\"merch_long\")))))\n",
    "                       ) * lit(6371.0)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cc_dist.select([\"lat\", \"long\", \"merch_lat\", \"merch_long\", \"dist_kms\"]).take(10), \n",
    "             columns=[\"lat\", \"long\", \"merch_lat\", \"merch_long\" , \"dist_kms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dist.select(\"dist_kms\").describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get histogram of fraudulent transactions by distance from home\n",
    "bins_dist, counts_dist = (cc_dist.where(col(\"is_fraud\")==1.0)\n",
    "                      .select(\"dist_kms\").rdd.flatMap(lambda x: x)\n",
    "                               .map(float).histogram(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize = (8,4))\n",
    "hist_dist = pd.DataFrame(zip(bins_dist,counts_dist), columns=['distance','count'])\n",
    "sns.barplot(hist_dist, x=\"distance\", y=\"count\").set(title='Fraud Count by Distance from Home')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "554b951f-367b-40b0-bad2-07dfa6bd2870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:16.350014Z",
     "start_time": "2023-11-21T14:47:16.339658Z"
    }
   },
   "outputs": [],
   "source": [
    "# define logarithmic transformer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only  # Note: use pyspark.ml.util.keyword_only if Spark < 2.0\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    " \n",
    "class LogTransformer(Transformer,               # Base class\n",
    "                     HasInputCol,               # Sets up an inputCol parameter\n",
    "                     HasOutputCol,              # Sets up an outputCol parameter\n",
    "                     DefaultParamsReadable,     # Makes parameters readable from file\n",
    "                     DefaultParamsWritable      # Makes parameters writable from file\n",
    "                    ):\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, append_str=None):\n",
    "        \"\"\"\n",
    "        Constructor: set values for all Param objects\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._setDefault()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "  \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        This is the main member function which applies the transform to transform data from the `inputCol` to the `outputCol`\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"LogTransformer transformer.\"\n",
    "            )\n",
    "        input_column = self.getInputCol()\n",
    "        output_column = self.getOutputCol()\n",
    "\n",
    "        return dataset.withColumn(output_column,\n",
    "                                  log(col(input_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:17.817197Z",
     "start_time": "2023-11-21T14:47:17.790339Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61929bd3-d3ca-4ead-8355-c7aa8510fff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# define a transformer to convert string categorical features to numeric indices\n",
    "inputs = ['merchant', 'category', 'gender', 'city', 'state', 'job']\n",
    "outputs = ['merchant_idx', 'category_idx', 'gender_idx', 'city_idx', 'state_idx', 'job_idx']\n",
    "stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:19.470612Z",
     "start_time": "2023-11-21T14:47:19.459432Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c330f6-3299-4f01-8be9-e093e31e0201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# define a transformer to one-hot encode indexed categorical features\n",
    "inputs_1hot = ['merchant_idx', 'category_idx', 'city_idx', 'state_idx', 'job_idx', 'hour_of_day']\n",
    "outputs_1hot = ['merchant_1hot', 'category_1hot', 'city_1hot', 'state_1hot', 'job_1hot', 'hour_of_day_1hot']\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=inputs_1hot, outputCols=outputs_1hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:20.898896Z",
     "start_time": "2023-11-21T14:47:20.888356Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7f0afc-bb1a-400e-8381-803b5d29655b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assemble the prepped features into one single vector.\n",
    "#featureCols = ['amt_log', 'city_pop_log', 'job_1hot', 'state_1hot', \n",
    "#               'category_1hot', 'gender_idx', 'hour_of_day_1hot', 'txns_last_hour']\n",
    "featureCols = ['amt_log', 'city_pop_log', 'job_1hot', 'state_1hot', 'txns_last_24h',\n",
    "               'category_1hot', 'gender_idx', 'hour_of_day_1hot', 'dist_kms']\n",
    "assembler = (VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\"))\n",
    "\n",
    "# cc_final = assembler.transform(cc_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:47:24.083278Z",
     "start_time": "2023-11-21T14:47:24.079561Z"
    }
   },
   "outputs": [],
   "source": [
    "amtTransformer = LogTransformer(inputCol=\"amt\", outputCol=\"amt_log\")\n",
    "cityPopTransformer = LogTransformer(inputCol=\"city_pop\", outputCol=\"city_pop_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "colTransformer = SQLTransformer(\n",
    "    statement = \"\"\"SELECT *, hour(trans_date_trans_time) hour_of_day,\n",
    "                             log(amt) amt_log,\n",
    "                             log(city_pop) city_pop_log\n",
    "                    FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "addWindow = SQLTransformer(\n",
    "    statement = \"\"\"SELECT *, window(trans_date_trans_time, '24 hours', '1 hour') time_window\n",
    "                    FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTest = addWindow.transform(cc)\n",
    "pd.DataFrame(myTest.take(10), columns=myTest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addCount = SQLTransformer(\n",
    "    statement = \"\"\"SELECT *, (SELECT count() FROM __THIS__ m2 \n",
    "                                where m2.cc_num = cc_num and m2.time_window = time_window) as txns_last24\n",
    "                   FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myCounts = addCount.transform(myTest)\n",
    "pd.DataFrame(myCounts.take(10), columns=myCounts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "dist = SQLTransformer(\n",
    "    statement = \"\"\"SELECT *, \n",
    "                    round((acos((sin(radians(lat)) * sin(radians(merch_lat))) + \n",
    "                   ((cos(radians(lat)) * cos(radians(merch_lat))) * \n",
    "                    (cos(radians(long) - radians(merch_long))))) * 6371.0), 0)\n",
    "                    as dist_kms                       \n",
    "                    FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "# calculate fraudulent transaction ratio\n",
    "fraudRatio = cc.filter(col(\"is_fraud\")==1).count() / cc.count()\n",
    "\n",
    "weightCalc = SQLTransformer(\n",
    "    statement = \"SELECT *, CASE WHEN is_fraud = 1 THEN \" + \n",
    "                str(1-fraudRatio) + \" ELSE \" + \n",
    "                str(fraudRatio) + \" END AS weight FROM __THIS__\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "# tsSec = SQLTransformer(\n",
    "#     statement = \"\"\"SELECT *, CAST(trans_date_trans_time AS LONG) AS ts_seconds\n",
    "#                     FROM __THIS__\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc_sec = tsSec.transform(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(cc_sec.take(20), columns=cc_sec.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "# txLast24 = SQLTransformer(\n",
    "#    statement = \"\"\"SELECT *, COUNT(*) OVER (PARTITION BY cc_num ORDER BY ts_seconds \n",
    "#                                RANGE BETWEEN 86400 PRECEDING AND CURRENT ROW) AS txns_last_24h\n",
    "#                    FROM __THIS__\"\"\"\n",
    "#)\n",
    "\n",
    "txLast24 = SQLTransformer(\n",
    "    statement = \"\"\"SELECT *, COUNT(*) OVER (PARTITION BY cc_num ORDER BY trans_date_trans_time \n",
    "                                RANGE BETWEEN INTERVAL 24 hours PRECEDING AND CURRENT ROW) AS txns_last_24h\n",
    "                    FROM __THIS__\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_last24 = txLast24.transform(cc)\n",
    "pd.DataFrame(cc_last24.take(10), columns=cc_last24.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# as our dataset is a time series, we do not want to randomly split it\n",
    "# so we will split by using the rank() function\n",
    "\n",
    "cc = cc.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"trans_date_trans_time\")))\n",
    "\n",
    "training = cc.where(\"rank <= .8\").drop(\"rank\")\n",
    "test = cc.where(\"rank > .8\").drop(\"rank\")\n",
    "\n",
    "# training, test = cc.randomSplit([0.7, 0.3])\n",
    "\n",
    "print(training.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_cc = weightCalc.transform(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(weighted_cc.take(10), columns=weighted_cc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = colTransformer.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(temp_df.take(10), columns=temp_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac03ef3e-ea18-4b83-b2f4-d815c04e81ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML Training and Prediction - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# cc = cc.withColumn('ts_seconds', col(\"trans_date_trans_time\").cast('long'))\n",
    "\n",
    "# w = (Window()\n",
    "#     .partitionBy(col(\"cc_num\"))\n",
    "#     .orderBy('ts_seconds')\n",
    "#     .rangeBetween(-60*60*24, Window.currentRow)\n",
    "#     )\n",
    "\n",
    "#cc = (cc\n",
    "#       .withColumn('txns_last_24h', count(\"*\").over(w))\n",
    "#       .orderBy(desc(\"ts_seconds\"))\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fraudulent transaction ratio\n",
    "# fraudRatio = cc.filter(col(\"is_fraud\")==1).count() / cc.count()\n",
    "# fraudRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_cc =  cc.withColumn(\"weight\", when(col(\"is_fraud\")==1.0, 1-fraudRatio).otherwise(fraudRatio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_cc =  weighted_cc.withColumn('hour_of_day', hour('trans_date_trans_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_cc = weighted_cc.withColumn('hour_of_day2', expr(\"hour(trans_date_trans_time)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(my_cc.where(col(\"cc_num\")=='630423337322').take(10), columns=my_cc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(training.where(col(\"txns_last_hour\")>5).take(10), columns=training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(training.where(col(\"cc_num\")==3573030041201292).take(20), columns=training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(training.where(col(\"is_fraud\")==1.0).groupBy(\"txns_last_hour\").count().head(10), columns=['txns_last_hour', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:36:35.187683Z",
     "start_time": "2023-11-15T19:36:10.439863Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862b103f-7fa2-4e8f-bb2c-392d684e3cff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(numTrees=10, maxDepth=5, labelCol=\"is_fraud\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "rf.setFeaturesCol(\"features\")\n",
    "rf.setWeightCol(\"weight\")\n",
    "\n",
    "# define pipeline using previously defined stages\n",
    "# pipeline = Pipeline(stages=[stringIndexer, oneHotEncoder, amtTransformer, cityPopTransformer, assembler, rf])\n",
    "pipeline = Pipeline(stages=[txLast24, colTransformer, dist,\n",
    "                            weightCalc, stringIndexer, oneHotEncoder, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:37:45.836414Z",
     "start_time": "2023-11-15T19:37:45.760815Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ae490b-dd6a-4a8d-a9c2-1a07021da0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = model.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "    list_extract = []\n",
    "    for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    varlist = pd.DataFrame(list_extract)\n",
    "    varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "    return(varlist.sort_values('score', ascending = False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ExtractFeatureImp(model.stages[-1].featureImportances, preds, \"features\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Now let's plot and set the xlim to not exceed 100%\n",
    "plt.figure(figsize=(9, 3))\n",
    "sns.barplot(x='score', y='name', data=features, orient='h')\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Feature Score')\n",
    "plt.ylabel('Feature')\n",
    "\n",
    "# Set x-axis limit to not exceed 1\n",
    "plt.xlim(0, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:39:59.731216Z",
     "start_time": "2023-11-15T19:39:57.820743Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692188e6-9c49-497b-8b56-b04bd83147e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "def extract_prob(v):\n",
    "    try:\n",
    "        return float(v[1])  # Your VectorUDT is of length 2\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "extract_prob_udf = udf(extract_prob, DoubleType())\n",
    "\n",
    "df2 = preds.withColumn(\"prob_1\", extract_prob_udf(col(\"probability\")))\n",
    "\n",
    "pd.DataFrame(df2.where(col(\"prediction\")==1).where(col(\"is_fraud\")==0).take(20), columns=df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"pred_adj\",when(col(\"prob_1\")>.65, 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df3.head(20), columns=df3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T20:26:50.892046Z",
     "start_time": "2023-11-15T20:26:38.182845Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c331dd2d-7945-4a22-98f0-f6569566f8e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "preds_float2 = df3 \\\n",
    "    .select(\"pred_adj\", \"is_fraud\") \\\n",
    "    .withColumn(\"is_fraud\", col(\"is_fraud\").cast(DoubleType())) \\\n",
    "    .orderBy(\"pred_adj\")\n",
    "\n",
    "cm3 = MulticlassMetrics(preds_float2.rdd.map(tuple))\n",
    "\n",
    "# print(cm.confusionMatrix().toArray())\n",
    "\n",
    "#show the confusion matrix as a pandas df for clearer presentation\n",
    "pd.DataFrame(cm3.confusionMatrix().toArray(),\n",
    "             columns= [\"predicted (0)\", \"predicted (1)\"],\n",
    "             index= [\"actual (0)\", \"actual (1)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print overall classification stats\n",
    "\n",
    "precision = cm3.precision(1.0)\n",
    "recall = cm3.recall(1.0)\n",
    "f1Score = cm3.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:44:25.045325Z",
     "start_time": "2023-11-15T19:44:19.726175Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c3bac3-aaeb-40d5-a104-254c44cab3b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Instantiate the evaluator\n",
    "bce= BinaryClassificationEvaluator(rawPredictionCol= \"rawPrediction\",\n",
    "                                   labelCol=\"is_fraud\", \n",
    "                                   metricName= \"areaUnderPR\")\n",
    "                                   \n",
    "bce.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T20:26:50.892046Z",
     "start_time": "2023-11-15T20:26:38.182845Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c331dd2d-7945-4a22-98f0-f6569566f8e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "preds_float = preds \\\n",
    "    .select(\"prediction\", \"is_fraud\") \\\n",
    "    .withColumn(\"is_fraud\", col(\"is_fraud\").cast(DoubleType())) \\\n",
    "    .orderBy(\"prediction\")\n",
    "\n",
    "cm = MulticlassMetrics(preds_float.rdd.map(tuple))\n",
    "\n",
    "# print(cm.confusionMatrix().toArray())\n",
    "\n",
    "#show the confusion matrix as a pandas df for clearer presentation\n",
    "pd.DataFrame(cm.confusionMatrix().toArray(),\n",
    "             columns= [\"predicted (0)\", \"predicted (1)\"],\n",
    "             index= [\"actual (0)\", \"actual (1)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print overall classification stats\n",
    "\n",
    "precision = cm.precision(1.0)\n",
    "recall = cm.recall(1.0)\n",
    "f1Score = cm.fMeasure(1.0)\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://stackoverflow.com/questions/52847408/pyspark-extract-roc-curve\n",
    "\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Scala version implements .roc() and .pr()\n",
    "# Python: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/common.html\n",
    "# Scala: https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html\n",
    "class CurveMetrics(BinaryClassificationMetrics):\n",
    "    def __init__(self, *args):\n",
    "        super(CurveMetrics, self).__init__(*args)\n",
    "\n",
    "    def _to_list(self, rdd):\n",
    "        points = []\n",
    "        # Note this collect could be inefficient for large datasets \n",
    "        # considering there may be one probability per datapoint (at most)\n",
    "        # The Scala version takes a numBins parameter, \n",
    "        # but it doesn't seem possible to pass this from Python to Java\n",
    "        for row in rdd.collect():\n",
    "            # Results are returned as type scala.Tuple2, \n",
    "            # which doesn't appear to have a py4j mapping\n",
    "            points += [(float(row._1()), float(row._2()))]\n",
    "        return points\n",
    "\n",
    "    def get_curve(self, method):\n",
    "        rdd = getattr(self._java_model, method)().toJavaRDD()\n",
    "        return self._to_list(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Returns as a list (false positive rate, true positive rate)\n",
    "preds_list = preds.select('is_fraud','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['is_fraud'])))\n",
    "points = CurveMetrics(preds_list).get_curve('pr')\n",
    "\n",
    "plt.figure()\n",
    "x_val = [x[0] for x in points]\n",
    "y_val = [x[1] for x in points]\n",
    "plt.title(\"PR Curve\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(x_val, y_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74111b8-28b8-432c-9013-c8a482d70d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results Analysis\n",
    "\n",
    "With a false negative rate of 100% in the confusion matrix, and 0.5 AUC score we obviously have work to do! ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction - Prototype 1 - File Based Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the following value to True if you want to test drive streaming\n",
    "enableStreaming = True\n",
    "# Set to true if running locally, false if run on Databricks\n",
    "isLocal = True\n",
    "\n",
    "inputPath = 'events' if isLocal else \"/Filestore/events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enableStreaming):\n",
    "\n",
    "    # Repartition the test data and break them down into 100 different files and write it to a csv file.\n",
    "    testData = test.repartition(100)\n",
    "    testData.write.mode(\"overwrite\").format(\"CSV\").option(\"header\",True).save(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish event stream\n",
    "if (enableStreaming):\n",
    "    events = spark.readStream.format(\"csv\") \\\n",
    "                             .option(\"header\",True) \\\n",
    "                             .schema(ccschema) \\\n",
    "                             .option(\"ignoreLeadingWhiteSpace\",True) \\\n",
    "                             .option(\"mode\",\"dropMalformed\") \\\n",
    "                             .option(\"maxFilesPerTrigger\",1) \\\n",
    "                             .load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enableStreaming):\n",
    "    # make predictions as new batches come in.\n",
    "    # to stop this process, interrupt the kernel with the stop button\n",
    "    predStream = model.transform(events).select(\"is_fraud\", \"probability\", \"prediction\")\n",
    "\n",
    "    # write to console as new batches come in\n",
    "    predStream.writeStream.format(\"console\").outputMode(\"append\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Time Prediction - Prototype 2 - Kafka Based Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enableKafka = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enableKafka):\n",
    "    df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"cc_fraud_topic\") \\\n",
    "        .load()\n",
    "\n",
    "# .option(\"startingOffsets\", \"earliest\") \\\n",
    "\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enableKafka):\n",
    "    stringDF = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_csv\n",
    "\n",
    "if (enableKafka):\n",
    "    events = stringDF.select(from_csv(col(\"value\"),ccschema.simpleString()).alias(\"data\")).select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccDF.writeStream \\\n",
    "#      .format(\"console\") \\\n",
    "#      .outputMode(\"append\") \\\n",
    "#      .start() \\\n",
    "#      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (enableKafka):\n",
    "    # make predictions as new batches come in.\n",
    "    # to stop this process, interrupt the kernel with the stop button\n",
    "    predStream = model.transform(events).select(\"trans_date_trans_time\", \"cc_num\", \"amt\", \"probability\", \"prediction\")\n",
    "\n",
    "    # write to console as new batches come in\n",
    "    predStream.writeStream.format(\"console\").outputMode(\"append\").start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cc_fraud_detection",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
