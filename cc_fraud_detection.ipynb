{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d2698b-7d95-497f-8d79-87170d247990",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Real-Time Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e026d554-cc9a-4ac3-9fe9-f4b94c6e1642",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:54:11.393751Z",
     "start_time": "2023-11-15T18:54:06.381027Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac2b174-0651-4698-a437-545dda582df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import modules from pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# uncomment the following line if running pyspark from the notebook itself\n",
    "# spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4fa66b5-0bd7-42b2-8002-0deffa9b601f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Loading the Dataset File and Performing Basic Data Type Conversions\n",
    "**Note**: Only run the following cells once or as required to reconstruct. Tables are persisted. Also, only run one of the following two cells, depending on whether you are running the notebook locally or from the shared workspace.\n",
    "\n",
    "Source of the dataset: https://www.kaggle.com/datasets/kartik2112/fraud-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "ccschema = StructType([\n",
    "    StructField(\"_c0\", IntegerType(), True),\n",
    "    StructField(\"trans_date_trans_time\", TimestampType(), True),\n",
    "    StructField(\"cc_num\", StringType(), True),\n",
    "    StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amt\", DoubleType(), True),\n",
    "    StructField(\"first\", StringType(), True),\n",
    "    StructField(\"last\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"street\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"long\", DoubleType(), True),\n",
    "    StructField(\"city_pop\", DoubleType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"trans_num\", StringType(), True),\n",
    "    StructField(\"unix_time\", StringType(), True),\n",
    "    StructField(\"merch_lat\", DoubleType(), True),\n",
    "    StructField(\"merch_long\", DoubleType(), True),\n",
    "    StructField(\"is_fraud\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78258743-1bd7-48ea-b54a-82f0637471e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data from local folder\n",
    "cc = (spark.read.csv(\"fraudTrain.csv\", schema=ccschema, header=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cc_raw.limit(10).write.option(\"header\",True).csv(\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662b3a2b-365c-4f39-8764-5d80d45093a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read The data in the context of Databricks\n",
    "# cc_raw = (spark.read\n",
    "#  .option(\"header\", \"true\")\n",
    "#  .csv(\"s3://group9-ml-project/fraudTrain.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:54:14.590933Z",
     "start_time": "2023-11-15T18:54:14.552974Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39dcadd2-22c6-422b-80b7-efb47ee63530",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register spark SQL tables\n",
    "\n",
    "# cc_raw.createOrReplaceTempView(\"cc_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:07.826517Z",
     "start_time": "2023-11-15T18:58:03.944148Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38621172-9e29-4672-a5c2-9f2bc637a66a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:17.122206Z",
     "start_time": "2023-11-15T18:58:09.050417Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebfd848-0075-4c2e-b883-be85b18a1caf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\" CREATE TABLE IF NOT EXISTS cc AS \n",
    "#                SELECT timestamp(trans_date_trans_time) as trans_date_trans_time,\n",
    "#                       cc_num, \n",
    "#                       merchant, \n",
    "#                       category,\n",
    "#                       double(amt) as amt,\n",
    "#                       first,\n",
    "#                       last,\n",
    "#                       gender,\n",
    "#                       street,\n",
    "#                       city,\n",
    "#                       state,\n",
    "#                       zip,\n",
    "#                       double(lat) as lat,\n",
    "#                       double(long) as long,\n",
    "#                       double(city_pop) as city_pop,\n",
    "#                       job,\n",
    "#                       date(dob) as dob,\n",
    "#                       trans_num,\n",
    "#                       unix_time,\n",
    "#                       double(merch_lat) as merch_lat,\n",
    "#                       double(merch_long) as merch_long,\n",
    "#                       int(is_fraud) as is_fraud\n",
    "#                  FROM cc_raw\n",
    "#          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "750eac0b-4eb7-48ad-961c-0ef45e6c85ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:18.703385Z",
     "start_time": "2023-11-15T18:58:18.588620Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f278df-8481-4941-8d3e-16fd7fe2ea87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Cache the table and define the main dataframe\n",
    "\n",
    "# sqlContext.cacheTable(\"cc\")\n",
    "\n",
    "# cc = spark.sql(\"SELECT * FROM cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b6fbbf-237e-4274-8e14-4ba90030b5ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### A Look at the Data and its Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:20.606359Z",
     "start_time": "2023-11-15T18:58:20.600302Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc0c630a-9595-4836-a7f7-b0bf2a2d0cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first look at the columns\n",
    "# cc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:31.291076Z",
     "start_time": "2023-11-15T18:58:24.486142Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae7e13a-f45b-4923-96bb-dfb19a7af27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# let's look a the first 10 rows\n",
    "pd.DataFrame(cc.take(10), columns=cc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:54:39.470599Z",
     "start_time": "2023-11-15T18:54:18.463395Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd519a3e-30d3-4409-9fe2-dadf0bf338a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:58:32.444351Z",
     "start_time": "2023-11-15T18:58:31.296283Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf09b72-f077-4f6c-9f08-1978f21b42dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# looking to see if there are null values\n",
    "cc.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in cc.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84faa957-4245-4c35-b435-6672c292af11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualizing the Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:02.649790Z",
     "start_time": "2023-11-15T18:58:35.118930Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352d619f-99a1-492c-8b18-9cebef04f7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4 , 2, figsize=(15, 20))\n",
    "fig.suptitle('CC Fraud Data Distribution')\n",
    "\n",
    "for idx, column in enumerate(['amt', 'city_pop', 'lat', 'long', 'merch_lat', 'merch_long', 'unix_time', 'is_fraud']):\n",
    "    # Show histogram of the column\n",
    "    bins, counts = cc.select(column).rdd.flatMap(lambda x: x).map(float).histogram(20)\n",
    "    axs[idx//2][idx%2].set_title(column)\n",
    "    axs[idx//2][idx%2].hist(bins[:-1], bins=bins, weights=counts)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:07.618644Z",
     "start_time": "2023-11-15T18:59:07.052600Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a728cb02-c2c8-4a85-92dc-0cd5ee9e0964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"category\").groupby(\"category\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:08.844594Z",
     "start_time": "2023-11-15T18:59:08.634036Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45d4906c-1a4f-4326-a063-ce1939ce9260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cc.select(\"gender\").groupby(\"gender\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fd7996-c3bd-4d19-8ac5-d1dfad4dc8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### EDA Preliminary Findings\n",
    "\n",
    "- The data appears to be clean with no missing values\n",
    "- Some of the heavily skewed features like amt and city_pop may benefit from logarithmic transformation\n",
    "- The target class (is_fraud) is heavily imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97e15c09-36da-402c-8bab-40e996db8e59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:38.391423Z",
     "start_time": "2023-11-15T18:59:38.363531Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedf9dd4-111d-4080-9f75-d77e5adf0e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply logarithmic transformation to amt and cit_pop features\n",
    "# cc = cc.withColumn('amt_log', log(col(\"amt\"))) \\\n",
    "#                                .withColumn('city_pop_log', log(col(\"city_pop\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:21.044312Z",
     "start_time": "2023-11-15T18:59:18.999942Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61929bd3-d3ca-4ead-8355-c7aa8510fff7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# convert string categorical features to numeric indices\n",
    "inputs = ['merchant', 'category', 'gender', 'city', 'state', 'job']\n",
    "outputs = ['merchant_idx', 'category_idx', 'gender_idx', 'city_idx', 'state_idx', 'job_idx']\n",
    "stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
    "\n",
    "#model = stringIndexer.fit(cc)\n",
    "#cc_idx = model.transform(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:24.603346Z",
     "start_time": "2023-11-15T18:59:24.363829Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78ce147-5fce-4384-bb5a-3dfbf47f3685",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cc_idx.take(10), columns=cc_idx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:31.378803Z",
     "start_time": "2023-11-15T18:59:31.192024Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c330f6-3299-4f01-8be9-e093e31e0201",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# one-hot encode indexed categorical features\n",
    "inputs_1hot = ['merchant_idx', 'category_idx', 'city_idx', 'state_idx', 'job_idx']\n",
    "outputs_1hot = ['merchant_1hot', 'category_1hot', 'city_1hot', 'state_1hot', 'job_1hot']\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(inputCols=inputs_1hot, outputCols=outputs_1hot)\n",
    "# model_1hot = oneHotEncoder.fit(cc_idx)\n",
    "# cc_prepped = model_1hot.transform(cc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:34.140363Z",
     "start_time": "2023-11-15T18:59:32.904283Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5f1dca-8c16-4e6f-a4c9-e78d5eb82ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cc_prepped.take(5), columns=cc_1hot.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b494af7-a82d-4b1a-915c-2376e863d710",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Review of Revised Distribution for Amt and City_Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:49.721922Z",
     "start_time": "2023-11-15T18:59:43.085369Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a43c145-5542-4540-88d9-bd2665025379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# plot revised data distribution\n",
    "fig, axs = plt.subplots(1 , 2, figsize=(15, 5))\n",
    "fig.suptitle('Prepped CC Data Distribution')\n",
    "\n",
    "for idx, column in enumerate(['amt_log', 'city_pop_log']):\n",
    "    # Show histogram of the column\n",
    "    bins, counts = cc.select(column).rdd.flatMap(lambda x: x).map(float).histogram(20)\n",
    "    axs[idx%2].set_title(column)\n",
    "    axs[idx%2].hist(bins[:-1], bins=bins, weights=counts)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "554b951f-367b-40b0-bad2-07dfa6bd2870",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Extraction and Training/Test DataSet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T18:59:56.680491Z",
     "start_time": "2023-11-15T18:59:56.610531Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7f0afc-bb1a-400e-8381-803b5d29655b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# assemble the prepped features into one single vector.\n",
    "featureCols = ['amt_log', 'city_pop_log', 'job_1hot', 'state_1hot', 'category_1hot', 'gender_idx']\n",
    "assembler = (VectorAssembler()\n",
    "  .setInputCols(featureCols)\n",
    "  .setOutputCol(\"features\"))\n",
    "\n",
    "# cc_final = assembler.transform(cc_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:00:09.663481Z",
     "start_time": "2023-11-15T18:59:59.143554Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a296f54-ca99-4cd5-96d1-50b702d00018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# setup training and test datasets\n",
    "\n",
    "# training, test = cc_final.randomSplit([0.7, 0.3])\n",
    "\n",
    "#  Not working with a dataset this size - Going to cache the data to make sure things stay snappy!\n",
    "# training.cache()\n",
    "# test.cache()\n",
    "\n",
    "# print(training.count())\n",
    "# print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T20:16:54.846672Z",
     "start_time": "2023-11-15T20:16:50.210523Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a823dd7e-a434-461d-b2cb-23af507fca4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test.select(\"is_fraud\").groupby(\"is_fraud\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac03ef3e-ea18-4b83-b2f4-d815c04e81ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML Training and Prediction - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logarithmic transformer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import keyword_only  # Note: use pyspark.ml.util.keyword_only if Spark < 2.0\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    " \n",
    "class LogTransformer(Transformer,               # Base class\n",
    "                     HasInputCol,               # Sets up an inputCol parameter\n",
    "                     HasOutputCol,              # Sets up an outputCol parameter\n",
    "                     DefaultParamsReadable,     # Makes parameters readable from file\n",
    "                     DefaultParamsWritable      # Makes parameters writable from file\n",
    "                    ):\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, append_str=None):\n",
    "        \"\"\"\n",
    "        Constructor: set values for all Param objects\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._setDefault()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "  \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setInputCol(self, new_inputCol):\n",
    "        return self.setParams(inputCol=new_inputCol)\n",
    "  \n",
    "    # Required if you use Spark >= 3.0\n",
    "    def setOutputCol(self, new_outputCol):\n",
    "        return self.setParams(outputCol=new_outputCol)\n",
    "  \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        This is the main member function which applies the transform to transform data from the `inputCol` to the `outputCol`\n",
    "        \"\"\"\n",
    "        if not self.isSet(\"inputCol\"):\n",
    "            raise ValueError(\n",
    "                \"No input column set for the \"\n",
    "                \"StringAppenderTransformer transformer.\"\n",
    "            )\n",
    "        input_column = self.getInputCol()\n",
    "        output_column = self.getOutputCol()\n",
    "        udf_func = lambda x: log(x)\n",
    "        data_type = DoubleType()\n",
    "         \n",
    "#        return dataset.withColumn(output_column,\n",
    "#                                  udf(udf_func, data_type)(col(input_column)))\n",
    "\n",
    "        return dataset.withColumn(output_column,\n",
    "                                  log(col(input_column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amtTransformer = LogTransformer(inputCol=\"amt\", outputCol=\"amt_log\")\n",
    "cityPopTransformer = LogTransformer(inputCol=\"city_pop\", outputCol=\"city_pop_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = cc.randomSplit([0.7, 0.3])\n",
    "\n",
    "print(training.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:36:35.187683Z",
     "start_time": "2023-11-15T19:36:10.439863Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862b103f-7fa2-4e8f-bb2c-392d684e3cff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"is_fraud\", seed=42,\n",
    "    leafCol=\"leafId\")\n",
    "rf.setFeaturesCol(\"features\")\n",
    "\n",
    "# define pipeline using previously defined stages\n",
    "pipeline = Pipeline(stages=[stringIndexer, oneHotEncoder, amtTransformer, cityPopTransformer, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:37:45.836414Z",
     "start_time": "2023-11-15T19:37:45.760815Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ae490b-dd6a-4a8d-a9c2-1a07021da0c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "preds = model.transform(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:39:59.731216Z",
     "start_time": "2023-11-15T19:39:57.820743Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692188e6-9c49-497b-8b56-b04bd83147e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(preds.take(10), columns=preds.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T19:44:25.045325Z",
     "start_time": "2023-11-15T19:44:19.726175Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c3bac3-aaeb-40d5-a104-254c44cab3b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Instantiate the evaluator\n",
    "bce= BinaryClassificationEvaluator(rawPredictionCol= \"rawPrediction\",\n",
    "                                   labelCol=\"is_fraud\", \n",
    "                                   metricName= \"areaUnderROC\")\n",
    "                                   \n",
    "bce.evaluate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T20:26:50.892046Z",
     "start_time": "2023-11-15T20:26:38.182845Z"
    },
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c331dd2d-7945-4a22-98f0-f6569566f8e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# create confusion matrix\n",
    "\n",
    "preds_float = preds \\\n",
    "    .select(\"prediction\", \"is_fraud\") \\\n",
    "    .withColumn(\"is_fraud\", col(\"is_fraud\").cast(DoubleType())) \\\n",
    "    .orderBy(\"prediction\")\n",
    "\n",
    "cm = MulticlassMetrics(preds_float.rdd.map(tuple))\n",
    "\n",
    "# print(cm.confusionMatrix().toArray())\n",
    "\n",
    "#show the confusion matrix as a pandas df for clearer presentation\n",
    "pd.DataFrame(cm.confusionMatrix().toArray(),\n",
    "             columns= [\"true positive\", \"true negative\"],\n",
    "             index= [\"predicted positive\", \"predicted negative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b74111b8-28b8-432c-9013-c8a482d70d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Results Analysis\n",
    "\n",
    "With a false negative rate of 100% in the confusion matrix, and 0.5 AUC score we obviously have work to do! ;-)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "cc_fraud_detection",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
